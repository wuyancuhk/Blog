---
layout:     post
title:      "Hadoop Learning Notes - 6"
subtitle:   " \"Hadoop - LSH and EMINIST\""
date:       2020.12.10 23:29:00
author:     "Wuy"
header-img: "img/post-bg-2020.jpg"
catalog: true
tags:

    - Hadoop

---


# Q1. Parameter Design for Minhash/ Locality-Sensitive Hashing (LSH)

## (a). [10 marks]

> *Derive the set of inequalities to govern the relationship between T1, T2, P1, P2, r and B so that the aforementioned accuracy/error requirements would be satisfied.*

$Let \; s \; be \;the \; Jaccard \; Similarity \; between \; two \; pairs,\; then: $
$$
L1(P1, P2, T1, T2, s)=
\begin{cases}
P1 \leqslant 1 - (1 - s^r)^B, \; s \geqslant T1  \\
P2 \geqslant 1 - (1 - s^r)^B, \; s \leqslant T2
\end{cases}
$$
$So,$
$$
L2(P1, P2, T1, T2)=
\begin{cases}
T1 \geqslant \sqrt[r]{[1 - \sqrt[b]{(1 - P1)}]} \\
T2 \leqslant \sqrt[r]{[1 - \sqrt[b]{(1 - P2)}]}
\end{cases}
$$

## (b). [10 marks]

> *For T1=0.85, T2=0.5, P1=0.99 and P2=0.01, use your results in part (a) to derive a single pair of values for (r, B) so that the aforementioned accuracy/error requirements would be satisfied. In general, there can be multiple feasible solutions. You only need to produce one of them. Show your steps (and source code if any).*

$To \; solve \; the \; following \;inequalities, $
$$
\begin{cases}
0.85 \geqslant \sqrt[r]{1 - \sqrt[b]{0.01}} \\
0.5 \leqslant \sqrt[r]{1 - \sqrt[b]{0.99}}
\end{cases}
$$
$I \; write \; a \; python \; program:$

```python
import sympy

b, r = sympy.symbols('b, r', nonenegative = True)

f1 = 1 - sympy.root(0.01, b)
f2 = sympy.root(f1, r)

f3 = 1 - sympy.root(0.99, b)
f4 = sympy.root(f3, r)

res = sympy.solveset(f2 <= 0.85, f4 >= 0.5)
print(res)
```

One possible result is:

$r = 20 \;, B = 120$

# Q2.k-means Clustering

## (a). [5 marks]

> *Before using k-means, we first need to determine the key parameter k, a.k.a, the number of clusters. Since we have 26 different labels corresponding to the 26 letters and every letter has uppercase and lowercase, the k parameter should be an integer between 26 and 52, please first choose a suitable k and explain why you choose that. (Hint: consider the similarity between uppercase and lowercase of the same letter.)*

I choose $k$ as 37.  

The reason is for letter *C, I, J, K, L, M, O, P, S, U, V, W, X, Y and Z*, their lower case and uppercase are similar, for the other 11 letters, their uppercase and lowercase are explicitly different from each other, so the $k$ should be $11 * 2 + 15 = 37$

## (b). [25 marks]

### Binary_converter

Firstly, I convert the raw files into vectors using python code as below:

```python
import numpy as np
import struct


def loadImageSet(filename):
    binfile = open(filename, 'rb')  # read the binary file
    buffers = binfile.read()

    head = struct.unpack_from('>IIII', buffers, 0)  # return a tuple for the first four objects

    offset = struct.calcsize('>IIII')  # locate the start point of the data
    imgNum = head[1]
    width = head[2]
    height = head[3]

    bits = imgNum * width * height  # there are 20800*28*28 pixels totally
    bitsString = '>' + str(bits) + 'B'  # format：'>16307200B'

    imgs = struct.unpack_from(bitsString, buffers, offset)  # return a tuple for the data

    binfile.close()
    imgs = np.reshape(imgs, [imgNum, width * height])  # reshape the data as a [20800,784] format array

    return imgs, head, offset


def loadLabelSet(filename):
    binfile = open(filename, 'rb')  # read the binary file
    buffers = binfile.read()

    head = struct.unpack_from('>II', buffers, 0)  # obtain the first two numbers of the label file

    labelNum = head[1]
    offset = struct.calcsize('>II')  # locate the start point of the data

    numString = '>' + str(labelNum) + "B"  # format：'>20800B'
    labels = struct.unpack_from(numString, buffers, offset)  # retrieve the data

    binfile.close()
    labels = np.reshape(labels, [labelNum])  # convert it to a 1-dimension array/vector

    return labels, head


if __name__ == "__main__":
    file1 = 'C:/Users/MSI-NB/AppData/Roaming/JetBrains/PyCharmCE2020.2/scratches/IERG4300_Homework_3/emnist-letters-test-images-idx3-ubyte'
    file2 = 'C:/Users/MSI-NB/AppData/Roaming/JetBrains/PyCharmCE2020.2/scratches/IERG4300_Homework_3/emnist-letters-test-labels-idx1-ubyte' # it's just a sample, I convert all the files including traning and test files

    imgs, data_head, offset = loadImageSet(file1)
    for i in range(len(imgs)):
        print(' '.join(map(str, imgs[i]))) # print the 20800 * 784 matrix into a .txt file
    # print('---------------------')
    # labels, labels_head = loadLabelSet(file2)
    # print(labels)
```

### KMeans

I am not familiar with multiple rounds of `MapReduce` using python, so I choose `java`. The `KMeans.java` is:

```java
import java.io.IOException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
public class KMeans {
	private static int num_clusters;
	private static int dimensionality;
	public static float[][] centroids ;
	public static void main(String[] args) throws Exception {
		int num_clusters = Integer.parseInt(args[0]);
		int dim = Integer.parseInt(args[1]);
		String input_path = args[2];
		String output_path = args[3];
		KMeans.setDimensionality(dim);
		KMeans.setNum_clusters(num_clusters);
		float[][] old_centroids = new float[num_clusters][dimensionality];
		initializeCentroids();
		while(!ifCentroidConverge(old_centroids,centroids)) {
			copyCentroidElements(old_centroids,centroids);
			Configuration conf = new Configuration();	
			Job job = Job.getInstance(conf, "k-means-mapReduce");
			job.setJarByClass(KMeans.class);
			job.setMapperClass(KMeansMapper.class);
			job.setCombinerClass(KMeansReducer.class);
			job.setReducerClass(KMeansReducer.class);
			job.setMapOutputKeyClass(IntWritable.class);
			job.setMapOutputValueClass(Text.class);
			job.setOutputKeyClass(IntWritable.class);
			job.setOutputValueClass(Text.class);
			deleteOutputDirectoryIfExist(conf,output_path);
			FileInputFormat.addInputPath(job, new Path(input_path));
			FileOutputFormat.setOutputPath(job, new Path(output_path));
			job.waitForCompletion(true);
		}
	}
	
	private static void copyCentroidElements(float[][] old_centroids, float[][] centroids) {
		for(int i =0;i<num_clusters;i++) {
			for(int j=0;j<dimensionality;j++) {
				old_centroids[i][j]=centroids[i][j];
			}
		}
	}
	public static void initializeCentroids() throws IOException{
			//initiate centroids for the first iteration only
			KMeans.centroids = new float[num_clusters][dimensionality];
			for(int i=0;i<num_clusters;i++) { 
				for(int j=0;j<dimensionality;j++) {
					int random_seed = (int) (Math.random() * 10);
					KMeans.centroids[i][j]=i+random_seed;
				}
		}
	}
	private static void deleteOutputDirectoryIfExist(Configuration conf, String output_path) throws IOException {

		Path outDir = new Path(output_path);
		FileSystem outFs = outDir.getFileSystem(conf);
		outFs.delete(outDir, true);
	}
	
	private static boolean ifCentroidConverge(float[][] old_centroids, float[][] centroids) {
		if(old_centroids==null ) {
			return false;
		}
		if(old_centroids.length != centroids.length) {
			return false;
		}
		for(int i =0;i<old_centroids.length;i++) {
			for(int j=0;j<centroids[0].length;j++) {
				if(old_centroids[i][j]!=centroids[i][j]) {
					//change detected, return false;
					return false;
				}
			}
		}

		return true;
	}
	public static void setNum_clusters(int num_cluster) {
		num_clusters = num_cluster;
	}
	public static int getNum_clusters() {
		return num_clusters;
	}
	public static void setDimensionality(int dimension) {
		dimensionality = dimension;
	}
	public static int getDimensionality() {
		return dimensionality;
	}
}
```



### KMeansMapper

Then the `KMeansMapper.java` is:

```java
import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class KMeansMapper extends Mapper<Object, Text, IntWritable, Text>{
	int num_clusters= KMeans.getNum_clusters();
	int dimensionality = KMeans.getDimensionality();
	private final static String SPACE = " ";
	@Override
	public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
		float min_distance = Float.MAX_VALUE;
		StringTokenizer itr = new StringTokenizer(value.toString());
		String pixelElements[] = new String[dimensionality];
		for(int i =0;i<dimensionality;i++) {
			pixelElements[i]=itr.nextToken();
		}
		int centroid_index=-1;	
		for(int i =0;i<num_clusters;i++) {
			float distance=-1;
			for(int j=0;j<dimensionality;j++) {
				distance += Math.pow(KMeans.centroids[i][j]-Float.parseFloat(pixelElements[j]),2);
			}
			distance = (float) Math.sqrt(distance);//calculate euclidean distance
			if(distance<min_distance) {
				min_distance=distance;
				centroid_index=i;
			}
		}
		IntWritable winner_centroid = new IntWritable(centroid_index);
		Text pixels = new Text();
		
		for(int k =0;k<pixelElements.length;k++) {
			pixels.append(pixelElements[k].getBytes(), 0, pixelElements[k].getBytes().length);
			pixels.append(SPACE.getBytes(), 0, 1);
		}
		context.write(winner_centroid, pixels);
	}
}

```

### KMeansReducer

The `KMeansReducer.java` is:

```java
import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class KMeansReducer extends Reducer<IntWritable,Text,IntWritable,Text> {
	private final static String space = " ";
	int num_clusters= KMeans.getNum_clusters();
	int dimension = KMeans.getDimensionality();
	private String[] result = new String[dimension];
	@Override
	public void reduce(IntWritable centroidKey, Iterable<Text> points, Context context) throws IOException, InterruptedException {
		int count=0;
		for(int i=0;i<result.length;i++) {
			result[i]= new String("0");
		}
		for (Text pixels : points) {
			count++;
			String[] indPixels = pixels.toString().split(" ");
			int i=0;
			for(String indPixel : indPixels) {
				result[i] = Float.toString(Float.parseFloat(result[i]) + Float.parseFloat(indPixel));
				i++;
			}
		}
		for(int i=0;i<result.length;i++) {
			float x=Float.parseFloat(result[i])/count;
			KMeans.centroids[Integer.parseInt(centroidKey.toString())][i] = x;
			result[i]= Float.toString(x);
		}
		Text centroidPixelsInText = new Text();
		prepareCentroidPixelsInText(centroidPixelsInText);
		context.write(centroidKey, centroidPixelsInText);
	}
	
	private void prepareCentroidPixelsInText(Text centroidPixelsInText) {
		for(int k =0;k<result.length;k++) {
			centroidPixelsInText.append(result[k].getBytes(), 0, result[k].getBytes().length);
			centroidPixelsInText.append(space.getBytes(), 0, 1);
		}
	}
}
```

### Hadoop Command

```shell
wget http://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip

unzip gzip.zip

cd gzip

gunzip *.gz

python3 binary_convert.py > train_img.txt

hadoop fs -put train_img.txt input

# after create KMeans.java/KMeansMapper.java/KMeansReducer.java
hadoop com.sun.tools.javac.Main *.java

jar cf kmeans.jar *.class

hadoop jar kmeans.jar KMeans 37 784 /user/1155148594/input/train_img.txt /user/1155148594/output
```



## (c). [20 marks]

Then  I use `python` to calculate the accuracy and converged result locally:

```python
#!/usr/bin/env python3

import numpy as np
from os.path import join
import struct as st
import copy
from tqdm import tqdm


class DataReader:

    def __init__(self, root_dir, type='emnist'):
        self.root_dir = root_dir
        self.type = type

    def get_train_data(self):
        filename = {'images': 'emnist-letters-train-images-idx3-ubyte',
                    'labels': 'emnist-letters-train-labels-idx1-ubyte'}
        labels_array = np.array([])
        data_types = {
            0x08: ('ubyte', 'B', 1),
            0x09: ('byte', 'b', 1),
            0x0B: ('>i2', 'h', 2),
            0x0C: ('>i4', 'i', 4),
            0x0D: ('>f4', 'f', 4),
            0x0E: ('>f8', 'd', 8)}
        for name in filename.keys():
            if name == 'images':
                imagesfile = open(join(self.root_dir, filename[name]), 'rb')
            if name == 'labels':
                labelsfile = open(join(self.root_dir, filename[name]), 'rb')
        imagesfile.seek(0)
        magic = st.unpack('>4B', imagesfile.read(4))
        if(magic[0] and magic[1])or(magic[2] not in data_types):
            raise ValueError("File Format not correct")
        nDim = magic[3]
        imagesfile.seek(4)
        nImg = st.unpack('>I', imagesfile.read(4))[0]  # num of images/labels
        nR = st.unpack('>I', imagesfile.read(4))[0]  # num of rows
        nC = st.unpack('>I', imagesfile.read(4))[0]  # num of columns
        nBytes = nImg*nR*nC
        # Since no. of items = no. of images and is already read
        labelsfile.seek(8)
        images_array = 255 - \
            np.asarray(st.unpack('>'+'B'*nBytes,
                                 imagesfile.read(nBytes))).reshape((nImg, nR, nC))
        labels_array = np.asarray(
            st.unpack('>'+'B'*nImg, labelsfile.read(nImg))).reshape((nImg, 1))
        labels_array = [l[0] for l in labels_array]
        return images_array.reshape(124800, 28*28), labels_array, None

class KMeans:

    def __init__(self, n_clusters=37, max_iter=500):
        self.n_clusters = n_clusters
        self.max_iter = max_iter
        self.loss_per_iteration = []

    def init_centroids(self):
        seed = np.random.randint(0,100000)
        print(seed) # print the random seed
        np.random.seed(seed)
        self.centroids = []
        for i in range(self.n_clusters):
            rand_index = np.random.choice(range(len(self.fit_data)))
            self.centroids.append(self.fit_data[rand_index])
            
    def init_clusters(self):
        self.clusters = {'data': {i: [] for i in range(self.n_clusters)}}
        self.clusters['labels'] = {i: [] for i in range(self.n_clusters)}

    def fit(self, fit_data, fit_labels):
        self.fit_data = fit_data
        self.fit_labels = fit_labels
        self.predicted_labels = [None for _ in range(self.fit_data.shape[0])]
        self.init_centroids()
        self.iterations = 0
        old_centroids = [np.zeros(shape=(fit_data.shape[1],))
                         for _ in range(self.n_clusters)]
        while not self.converged(self.iterations, old_centroids, self.centroids):
            old_centroids = copy.deepcopy(self.centroids)
            self.init_clusters()
            for j, sample in tqdm(enumerate(self.fit_data)):
                min_dist = float('inf')
                for i, centroid in enumerate(self.centroids):
                    dist = np.linalg.norm(sample-centroid)
                    if dist < min_dist:
                        min_dist = dist
                        self.predicted_labels[j] = i
                if self.predicted_labels[j] is not None:
                    self.clusters['data'][self.predicted_labels[j]].append(
                        sample)
                    self.clusters['labels'][self.predicted_labels[j]].append(
                        self.fit_labels[j])
            self.reshape_cluster()
            self.update_centroids()
            self.calculate_loss()
            print("\nIteration:", self.iterations, 'Loss:',
                  self.loss, 'Difference:', self.centroids_dist)
            self.iterations += 1
        self.calculate_accuracy()

    def update_centroids(self):
        for i in range(self.n_clusters):
            cluster = self.clusters['data'][i]
            if cluster == []:
                self.centroids[i] = self.fit_data[np.random.choice(
                    range(len(self.fit_data)))]
            else:
                self.centroids[i] = np.mean(
                    np.vstack((self.centroids[i], cluster)), axis=0)

    def reshape_cluster(self):
        for id, mat in list(self.clusters['data'].items()):
            self.clusters['data'][id] = np.array(mat)

    def converged(self, iterations, centroids, updated_centroids):
        if iterations > self.max_iter:
            return True
        self.centroids_dist = np.linalg.norm(
            np.array(updated_centroids)-np.array(centroids))
        if self.centroids_dist <= 1e-10:
            print("Converged! With distance:", self.centroids_dist)
            return True
        return False

    def calculate_loss(self):
        self.loss = 0
        for key, value in list(self.clusters['data'].items()):
            if value is not None:
                for v in value:
                    self.loss += np.linalg.norm(v-self.centroids[key])
        self.loss_per_iteration.append(self.loss)

    def calculate_accuracy(self):
        self.clusters_labels = []
        self.clusters_info = []
        self.clusters_accuracy = []
        for clust, labels in list(self.clusters['labels'].items()):
            if isinstance(labels[0], (np.ndarray)):
                labels = [l[0] for l in labels]
            occur = 0
            max_label = max(set(labels), key=labels.count)
            self.clusters_labels.append(max_label)
            for label in labels:
                if label == max_label:
                    occur += 1
            acc = occur/len(list(labels))
            self.clusters_info.append(
                [max_label, occur, len(list(labels)), acc])
            self.clusters_accuracy.append(acc)
            self.accuracy = sum(self.clusters_accuracy)/self.n_clusters
        self.labels_ = []
        for i in range(len(self.predicted_labels)):
            self.labels_.append(self.clusters_labels[self.predicted_labels[i]])
        print('[cluster_label,no_occurence_of_label,total_samples_in_cluster,cluster_accuracy]', self.clusters_info)
        print('Accuracy:', self.accuracy)


data_reader = DataReader('/home/1155148594/Homework3/letters/')
tr_data, tr_class_labels, tr_subclass_labels = data_reader.get_train_data()
kmeans = KMeans(n_clusters=37,max_iter=500)
kmeans.fit(tr_data,tr_class_labels)
```

I have run it for three times with different random seed each time, the final result is shown below:

### Table one with random seed 28218

The final output is :

```
Iteration: 0 Loss: 221723137.867707 Difference: 38356.60604120234

Iteration: 1 Loss: 214816335.5079826 Difference: 8116.00697127122

Iteration: 2 Loss: 212432192.9545585 Difference: 1569.664235576111
...
Iteration: 135 Loss: 208080261.52271414 Difference: 0.00017603638954752024

Iteration: 136 Loss: 208080261.52271414 Difference: 4.5801130822748886e-08
Converged! With distance: 1.209873396263341e-11
[cluster_label,no_occurence_of_label,total_samples_in_cluster,cluster_accuracy] [[5, 1860, 3003, 0.6193806193806194], [3, 708, 2958, 0.23935091277890466], [6, 1500, 4207, 0.3565486094604231], [23, 3013, 3640, 0.8277472527472527], [23, 789, 4459, 0.17694550347611573], [16, 1816, 3781, 0.4802962179317641], [11, 2008, 2963, 0.6776915288558893], [15, 2065, 2889, 0.7147802007615092], [18, 2227, 3757, 0.5927601809954751], [4, 1227, 3675, 0.33387755102040817], [13, 3328, 3767, 0.8834616405627821], [24, 2206, 3281, 0.6723559890277354], [25, 1239, 5168, 0.23974458204334365], [9, 1716, 5078, 0.33792831823552577], [6, 1037, 2761, 0.3755885548714234], [15, 1549, 3481, 0.4449870726802643], [10, 1536, 3706, 0.41446303291958986], [14, 1240, 4276, 0.2899906454630496], [26, 1632, 2087, 0.781983708672736], [26, 1683, 2167, 0.7766497461928934], [21, 1838, 2813, 0.6533949520085318], [10, 1408, 2658, 0.5297215951843491], [3, 1418, 3035, 0.4672158154859967], [17, 957, 2568, 0.3726635514018692], [14, 1729, 2791, 0.6194912217843067], [8, 1058, 3130, 0.3380191693290735], [8, 897, 2779, 0.3227779776898165], [7, 882, 2441, 0.36132732486685787], [17, 968, 3073, 0.31500162707452], [16, 856, 3692, 0.23185265438786565], [3, 597, 3607, 0.1655115054061547], [22, 1942, 2983, 0.651022460610124], [1, 1136, 3801, 0.2988687187582215], [8, 903, 3223, 0.28017375116351223], [25, 1169, 3475, 0.3364028776978417], [19, 2476, 3053, 0.8110055682934818], [9, 1605, 4574, 0.3508963707914298]]
Accuracy: 0.4686994327030177
```

Using the following `python` script to transfer above output to a `.csv` file and make use of https://tableconvert.com/ to transfer it to a `markdown` format:

```python
import csv

f = open("round1.txt")
with open("round1.csv", "w", newline="") as datacsv:
    csvwriter = csv.writer(datacsv, dialect=("excel"))
    count = 0
    for line in f.readlines():
        count += 1
        line = line.strip().split(",")
        train = line[2]
        label = line[0]
        correct = line[1]
        acc = line[3]
        acc = format(float(acc), ".2%")
        tmp = [count, train, label, correct, acc]
        csvwriter.writerow(tmp)
```



| Cluster Number | \# train images belongs to the cluster | Label of the cluster | \# correctly clustered images | Classification Accuracy |
| -------------- | -------------------------------------- | -------------------- | ----------------------------- | ----------------------- |
| 1              | 3003                                   | 5                    | 1860                          | 61\.94%                 |
| 2              | 2958                                   | 3                    | 708                           | 23\.94%                 |
| 3              | 4207                                   | 6                    | 1500                          | 35\.65%                 |
| 4              | 3640                                   | 23                   | 3013                          | 82\.77%                 |
| 5              | 4459                                   | 23                   | 789                           | 17\.69%                 |
| 6              | 3781                                   | 16                   | 1816                          | 48\.03%                 |
| 7              | 2963                                   | 11                   | 2008                          | 67\.77%                 |
| 8              | 2889                                   | 15                   | 2065                          | 71\.48%                 |
| 9              | 3757                                   | 18                   | 2227                          | 59\.28%                 |
| 10             | 3675                                   | 4                    | 1227                          | 33\.39%                 |
| 11             | 3767                                   | 13                   | 3328                          | 88\.35%                 |
| 12             | 3281                                   | 24                   | 2206                          | 67\.24%                 |
| 13             | 5168                                   | 25                   | 1239                          | 23\.97%                 |
| 14             | 5078                                   | 9                    | 1716                          | 33\.79%                 |
| 15             | 2761                                   | 6                    | 1037                          | 37\.56%                 |
| 16             | 3481                                   | 15                   | 1549                          | 44\.50%                 |
| 17             | 3706                                   | 10                   | 1536                          | 41\.45%                 |
| 18             | 4276                                   | 14                   | 1240                          | 29\.00%                 |
| 19             | 2087                                   | 26                   | 1632                          | 78\.20%                 |
| 20             | 2167                                   | 26                   | 1683                          | 77\.66%                 |
| 21             | 2813                                   | 21                   | 1838                          | 65\.34%                 |
| 22             | 2658                                   | 10                   | 1408                          | 52\.97%                 |
| 23             | 3035                                   | 3                    | 1418                          | 46\.72%                 |
| 24             | 2568                                   | 17                   | 957                           | 37\.27%                 |
| 25             | 2791                                   | 14                   | 1729                          | 61\.95%                 |
| 26             | 3130                                   | 8                    | 1058                          | 33\.80%                 |
| 27             | 2779                                   | 8                    | 897                           | 32\.28%                 |
| 28             | 2441                                   | 7                    | 882                           | 36\.13%                 |
| 29             | 3073                                   | 17                   | 968                           | 31\.50%                 |
| 30             | 3692                                   | 16                   | 856                           | 23\.19%                 |
| 31             | 3607                                   | 3                    | 597                           | 16\.55%                 |
| 32             | 2983                                   | 22                   | 1942                          | 65\.10%                 |
| 33             | 3801                                   | 1                    | 1136                          | 29\.89%                 |
| 34             | 3223                                   | 8                    | 903                           | 28\.02%                 |
| 35             | 3475                                   | 25                   | 1169                          | 33\.64%                 |
| 36             | 3053                                   | 19                   | 2476                          | 81\.10%                 |
| 37             | 4574                                   | 9                    | 1605                          | 35\.09%                 |
| Total set      | 124800                                 | NA                   | 56218                         | 46.87%                  |

### Table two with random seed 26601

The final output is:

```
Iteration: 0 Loss: 223111890.75220487 Difference: 38686.83054994296

Iteration: 1 Loss: 215122793.49284005 Difference: 8300.654411160573

Iteration: 2 Loss: 212521754.2775848 Difference: 1699.3001054325512
...
Iteration: 194 Loss: 208232779.2358123 Difference: 0.5398906174324227

Iteration: 195 Loss: 208232779.23581266 Difference: 0.00012893382874548857

Iteration: 196 Loss: 208232779.2358126 Difference: 3.1748399007268415e-08
Converged! With distance: 8.017421639892969e-12
[cluster_label,no_occurence_of_label,total_samples_in_cluster,cluster_accuracy] [[1, 741, 3162, 0.23434535104364326], [16, 1817, 3841, 0.47305389221556887], [3, 902, 3093, 0.29162625282896865], [19, 2441, 3086, 0.7909915748541801], [9, 2018, 5600, 0.3603571428571429], [23, 2959, 3530, 0.8382436260623229], [14, 1201, 3987, 0.30122899423125155], [13, 826, 4061, 0.203398177788722], [13, 3133, 3486, 0.8987378083763626], [14, 1626, 2557, 0.6359014470082127], [6, 1051, 2800, 0.37535714285714283], [26, 1572, 2328, 0.6752577319587629], [21, 798, 2311, 0.3453050627434011], [5, 1939, 2917, 0.6647240315392526], [10, 1673, 4592, 0.3643292682926829], [16, 817, 3886, 0.21024189397838394], [15, 1776, 3377, 0.5259105715131773], [6, 1524, 4599, 0.3313763861709067], [22, 1335, 2134, 0.6255857544517338], [26, 1726, 2249, 0.7674522009782125], [22, 1736, 2541, 0.6831955922865014], [8, 904, 3280, 0.275609756097561], [21, 923, 2870, 0.321602787456446], [7, 724, 3041, 0.23807957908582703], [3, 1435, 2836, 0.5059943582510579], [25, 1181, 3333, 0.35433543354335434], [4, 1591, 3657, 0.43505605687722176], [18, 2205, 3897, 0.5658198614318707], [19, 1035, 2946, 0.3513238289205703], [24, 1901, 5138, 0.3699883223043986], [17, 1144, 2802, 0.40827980014275517], [15, 1825, 2766, 0.6597975415762835], [8, 1075, 3273, 0.32844485181790406], [21, 1464, 2775, 0.5275675675675676], [9, 1740, 5997, 0.29014507253626814], [11, 1823, 3338, 0.5461354104254045], [8, 908, 2714, 0.33456153279292555]]
Accuracy: 0.4624151801314582
```

| Cluster Number | \# train images belongs to the cluster | Label of the cluster | \# correctly clustered images | Classification Accuracy |
| -------------- | -------------------------------------- | -------------------- | ----------------------------- | ----------------------- |
| 1              | 3162                                   | 1                    | 741                           | 23\.43%                 |
| 2              | 3841                                   | 16                   | 1817                          | 47\.31%                 |
| 3              | 3093                                   | 3                    | 902                           | 29\.16%                 |
| 4              | 3086                                   | 19                   | 2441                          | 79\.10%                 |
| 5              | 5600                                   | 9                    | 2018                          | 36\.04%                 |
| 6              | 3530                                   | 23                   | 2959                          | 83\.82%                 |
| 7              | 3987                                   | 14                   | 1201                          | 30\.12%                 |
| 8              | 4061                                   | 13                   | 826                           | 20\.34%                 |
| 9              | 3486                                   | 13                   | 3133                          | 89\.87%                 |
| 10             | 2557                                   | 14                   | 1626                          | 63\.59%                 |
| 11             | 2800                                   | 6                    | 1051                          | 37\.54%                 |
| 12             | 2328                                   | 26                   | 1572                          | 67\.53%                 |
| 13             | 2311                                   | 21                   | 798                           | 34\.53%                 |
| 14             | 2917                                   | 5                    | 1939                          | 66\.47%                 |
| 15             | 4592                                   | 10                   | 1673                          | 36\.43%                 |
| 16             | 3886                                   | 16                   | 817                           | 21\.02%                 |
| 17             | 3377                                   | 15                   | 1776                          | 52\.59%                 |
| 18             | 4599                                   | 6                    | 1524                          | 33\.14%                 |
| 19             | 2134                                   | 22                   | 1335                          | 62\.56%                 |
| 20             | 2249                                   | 26                   | 1726                          | 76\.75%                 |
| 21             | 2541                                   | 22                   | 1736                          | 68\.32%                 |
| 22             | 3280                                   | 8                    | 904                           | 27\.56%                 |
| 23             | 2870                                   | 21                   | 923                           | 32\.16%                 |
| 24             | 3041                                   | 7                    | 724                           | 23\.81%                 |
| 25             | 2836                                   | 3                    | 1435                          | 50\.60%                 |
| 26             | 3333                                   | 25                   | 1181                          | 35\.43%                 |
| 27             | 3657                                   | 4                    | 1591                          | 43\.51%                 |
| 28             | 3897                                   | 18                   | 2205                          | 56\.58%                 |
| 29             | 2946                                   | 19                   | 1035                          | 35\.13%                 |
| 30             | 5138                                   | 24                   | 1901                          | 37\.00%                 |
| 31             | 2802                                   | 17                   | 1144                          | 40\.83%                 |
| 32             | 2766                                   | 15                   | 1825                          | 65\.98%                 |
| 33             | 3273                                   | 8                    | 1075                          | 32\.84%                 |
| 34             | 2775                                   | 21                   | 1464                          | 52\.76%                 |
| 35             | 5997                                   | 9                    | 1740                          | 29\.01%                 |
| 36             | 3338                                   | 11                   | 1823                          | 54\.61%                 |
| 37             | 2714                                   | 8                    | 908                           | 33\.46%                 |
| Total Set      | 124800                                 | NA                   | 55489                         | 46.24%                  |

### Table three with random seed 45713

The final output is:

```
Iteration: 0 Loss: 222348498.3283384 Difference: 38563.496236726256
124800it [01:36, 1293.60it/s]

Iteration: 1 Loss: 214948312.34422886 Difference: 8082.938199944702
124800it [01:17, 1612.07it/s]
...
Iteration: 153 Loss: 208222789.38474783 Difference: 0.0004074507735720122
124800it [01:35, 1304.74it/s]

Iteration: 154 Loss: 208222789.3847479 Difference: 1.6447141933396244e-07
Converged! With distance: 7.040963510390078e-11
[cluster_label,no_occurence_of_label,total_samples_in_cluster,cluster_accuracy] [[9, 1589, 4933, 0.3221163592134604], [2, 847, 2845, 0.2977152899824253], [17, 1013, 2346, 0.4317988064791134], [15, 1784, 2804, 0.6362339514978602], [14, 1093, 4521, 0.24176067241760674], [13, 3212, 3587, 0.8954558126568163], [19, 2485, 3128, 0.7944373401534527], [22, 1815, 2906, 0.6245698554714384], [3, 882, 3287, 0.2683297839975662], [4, 2030, 4967, 0.4086974028588685], [24, 2292, 3378, 0.6785079928952042], [10, 909, 2501, 0.3634546181527389], [21, 1949, 3004, 0.6488015978695073], [13, 769, 4021, 0.19124595871673714], [16, 1711, 5287, 0.32362398335540005], [26, 880, 3209, 0.2742287316921159], [7, 747, 2941, 0.25399523971438287], [5, 1961, 2962, 0.662052667116813], [16, 1320, 3281, 0.4023163669612923], [23, 1883, 2430, 0.7748971193415638], [8, 1421, 3827, 0.37130911941468514], [26, 1912, 2248, 0.8505338078291815], [23, 1824, 2477, 0.7363746467501009], [21, 837, 2268, 0.36904761904761907], [25, 1179, 3314, 0.35576342788171395], [22, 1352, 2217, 0.6098331078033379], [26, 1072, 2750, 0.38981818181818184], [14, 1754, 2912, 0.6023351648351648], [25, 1149, 5017, 0.22902132748654574], [18, 2291, 3589, 0.6383393702981331], [3, 1467, 2804, 0.5231811697574893], [8, 1132, 3937, 0.2875285750571501], [9, 1741, 5383, 0.32342559910830393], [16, 890, 4026, 0.22106308991554893], [7, 859, 3245, 0.26471494607087825], [15, 1886, 3501, 0.538703227649243], [11, 1980, 2947, 0.671869697997964]]
Accuracy: 0.47235409808825946
```

| Cluster Number | \# train images belongs to the cluster | Label of the cluster | \# correctly clustered images | Classification Accuracy |
| -------------- | -------------------------------------- | -------------------- | ----------------------------- | ----------------------- |
| 1              | 4933                                   | 9                    | 1589                          | 32\.21%                 |
| 2              | 2845                                   | 2                    | 847                           | 29\.77%                 |
| 3              | 2346                                   | 17                   | 1013                          | 43\.18%                 |
| 4              | 2804                                   | 15                   | 1784                          | 63\.62%                 |
| 5              | 4521                                   | 14                   | 1093                          | 24\.18%                 |
| 6              | 3587                                   | 13                   | 3212                          | 89\.55%                 |
| 7              | 3128                                   | 19                   | 2485                          | 79\.44%                 |
| 8              | 2906                                   | 22                   | 1815                          | 62\.46%                 |
| 9              | 3287                                   | 3                    | 882                           | 26\.83%                 |
| 10             | 4967                                   | 4                    | 2030                          | 40\.87%                 |
| 11             | 3378                                   | 24                   | 2292                          | 67\.85%                 |
| 12             | 2501                                   | 10                   | 909                           | 36\.35%                 |
| 13             | 3004                                   | 21                   | 1949                          | 64\.88%                 |
| 14             | 4021                                   | 13                   | 769                           | 19\.12%                 |
| 15             | 5287                                   | 16                   | 1711                          | 32\.36%                 |
| 16             | 3209                                   | 26                   | 880                           | 27\.42%                 |
| 17             | 2941                                   | 7                    | 747                           | 25\.40%                 |
| 18             | 2962                                   | 5                    | 1961                          | 66\.21%                 |
| 19             | 3281                                   | 16                   | 1320                          | 40\.23%                 |
| 20             | 2430                                   | 23                   | 1883                          | 77\.49%                 |
| 21             | 3827                                   | 8                    | 1421                          | 37\.13%                 |
| 22             | 2248                                   | 26                   | 1912                          | 85\.05%                 |
| 23             | 2477                                   | 23                   | 1824                          | 73\.64%                 |
| 24             | 2268                                   | 21                   | 837                           | 36\.90%                 |
| 25             | 3314                                   | 25                   | 1179                          | 35\.58%                 |
| 26             | 2217                                   | 22                   | 1352                          | 60\.98%                 |
| 27             | 2750                                   | 26                   | 1072                          | 38\.98%                 |
| 28             | 2912                                   | 14                   | 1754                          | 60\.23%                 |
| 29             | 5017                                   | 25                   | 1149                          | 22\.90%                 |
| 30             | 3589                                   | 18                   | 2291                          | 63\.83%                 |
| 31             | 2804                                   | 3                    | 1467                          | 52\.32%                 |
| 32             | 3937                                   | 8                    | 1132                          | 28\.75%                 |
| 33             | 5383                                   | 9                    | 1741                          | 32\.34%                 |
| 34             | 4026                                   | 16                   | 890                           | 22\.11%                 |
| 35             | 3245                                   | 7                    | 859                           | 26\.47%                 |
| 36             | 3501                                   | 15                   | 1886                          | 53\.87%                 |
| 37             | 2947                                   | 11                   | 1980                          | 67\.19%                 |
| Total Set      | 124800                                 | NA                   | 55917                         | 47.23%                  |

### Comparision

From the above three tables, I found that with random seed 45713, the iteration rounds is between all three result, and the accuracy is the largest, so I choose 45713 as the best random seed.

## (d). [10 marks]

Using the same code above, and just modify the random seed as 45713 and the first argument in `reshape()` as 20800. The final output is:

```
Iteration: 0 Loss: 36620239.725016035 Difference: 39134.40874729041

Iteration: 1 Loss: 35649891.15421286 Difference: 7815.554912650023

...

Iteration: 93 Loss: 34604598.29312757 Difference: 7.847331801584406e-08

Iteration: 94 Loss: 34604598.29312757 Difference: 1.8544161877497954e-10

Converged! With distance: 6.170686287758597e-13
[cluster_label,no_occurence_of_label,total_samples_in_cluster,cluster_accuracy] [[17, 191, 416, 0.45913461538461536], [17, 137, 491, 0.2790224032586558], [3, 155, 693, 0.22366522366522368], [15, 326, 505, 0.6455445544554456], [1, 134, 602, 0.22259136212624583], [10, 200, 406, 0.49261083743842365], [9, 241, 803, 0.3001245330012453], [11, 287, 467, 0.6145610278372591], [4, 267, 496, 0.5383064516129032], [23, 575, 703, 0.817923186344239], [14, 216, 711, 0.3037974683544304], [18, 203, 462, 0.4393939393939394], [22, 261, 502, 0.5199203187250996], [15, 297, 636, 0.4669811320754717], [9, 283, 871, 0.3249138920780712], [12, 159, 788, 0.2017766497461929], [19, 387, 512, 0.755859375], [16, 285, 549, 0.5191256830601093], [2, 182, 536, 0.33955223880597013], [14, 274, 466, 0.5879828326180258], [24, 374, 540, 0.6925925925925925], [7, 123, 537, 0.22905027932960895], [3, 148, 494, 0.29959514170040485], [18, 230, 408, 0.5637254901960784], [25, 193, 504, 0.38293650793650796], [26, 357, 436, 0.8188073394495413], [13, 605, 682, 0.8870967741935484], [5, 324, 575, 0.5634782608695652], [8, 156, 500, 0.312], [16, 212, 594, 0.3569023569023569], [10, 306, 684, 0.4473684210526316], [1, 148, 639, 0.23161189358372458], [8, 166, 559, 0.29695885509839], [20, 161, 693, 0.23232323232323232], [22, 244, 345, 0.7072463768115942], [8, 151, 475, 0.3178947368421053], [21, 349, 520, 0.6711538461538461]]
Accuracy: 0.46117648189235944
```

| Cluster Number | \# train images belongs to the cluster | Label of the cluster | \# correctly clustered images | Classification Accuracy |
| -------------- | -------------------------------------- | -------------------- | ----------------------------- | ----------------------- |
| 1              | 416                                    | 17                   | 191                           | 45\.91%                 |
| 2              | 491                                    | 17                   | 137                           | 27\.90%                 |
| 3              | 693                                    | 3                    | 155                           | 22\.37%                 |
| 4              | 505                                    | 15                   | 326                           | 64\.55%                 |
| 5              | 602                                    | 1                    | 134                           | 22\.26%                 |
| 6              | 406                                    | 10                   | 200                           | 49\.26%                 |
| 7              | 803                                    | 9                    | 241                           | 30\.01%                 |
| 8              | 467                                    | 11                   | 287                           | 61\.46%                 |
| 9              | 496                                    | 4                    | 267                           | 53\.83%                 |
| 10             | 703                                    | 23                   | 575                           | 81\.79%                 |
| 11             | 711                                    | 14                   | 216                           | 30\.38%                 |
| 12             | 462                                    | 18                   | 203                           | 43\.94%                 |
| 13             | 502                                    | 22                   | 261                           | 51\.99%                 |
| 14             | 636                                    | 15                   | 297                           | 46\.70%                 |
| 15             | 871                                    | 9                    | 283                           | 32\.49%                 |
| 16             | 788                                    | 12                   | 159                           | 20\.18%                 |
| 17             | 512                                    | 19                   | 387                           | 75\.59%                 |
| 18             | 549                                    | 16                   | 285                           | 51\.91%                 |
| 19             | 536                                    | 2                    | 182                           | 33\.96%                 |
| 20             | 466                                    | 14                   | 274                           | 58\.80%                 |
| 21             | 540                                    | 24                   | 374                           | 69\.26%                 |
| 22             | 537                                    | 7                    | 123                           | 22\.91%                 |
| 23             | 494                                    | 3                    | 148                           | 29\.96%                 |
| 24             | 408                                    | 18                   | 230                           | 56\.37%                 |
| 25             | 504                                    | 25                   | 193                           | 38\.29%                 |
| 26             | 436                                    | 26                   | 357                           | 81\.88%                 |
| 27             | 682                                    | 13                   | 605                           | 88\.71%                 |
| 28             | 575                                    | 5                    | 324                           | 56\.35%                 |
| 29             | 500                                    | 8                    | 156                           | 31\.20%                 |
| 30             | 594                                    | 16                   | 212                           | 35\.69%                 |
| 31             | 684                                    | 10                   | 306                           | 44\.74%                 |
| 32             | 639                                    | 1                    | 148                           | 23\.16%                 |
| 33             | 559                                    | 8                    | 166                           | 29\.70%                 |
| 34             | 693                                    | 20                   | 161                           | 23\.23%                 |
| 35             | 345                                    | 22                   | 244                           | 70\.72%                 |
| 36             | 475                                    | 8                    | 151                           | 31\.79%                 |
| 37             | 520                                    | 21                   | 349                           | 67\.12%                 |
| Total Set      | 20800                                  | NA                   | 9307                          | 46.12%                  |

# Q3. Bernoulli Mixture Models

## (a). [20 marks]

> *Provide the pseudo code (MapReduce is NOT required) to estimate the parameters of a BMM model based on maximum-likelihood arguments using the Expectation-Maximization algorithm. The pseudo code should include detailed description on the list of input/ intermediate variables used and how each of them get updated during the E-step and M-step of each iteration.*

Pseudo code show below:

Mapper:
For each line (digit image):
	E Step, compute score, based on the previous result:

​	![image-20201123224820633](https://i.loli.net/2020/11/23/iVqChz3jnW7ldKk.png)

​	Emit(k,y($z_{nk}$))



Reducer:
		For each line (obtained by the reducer):
					M Step, re-compute the pi(k) and q(k):

![image-20201123225046007](https://i.loli.net/2020/11/23/yxFNBT2w7ZeJKO8.png)

![image-20201123225114541](https://i.loli.net/2020/11/23/67W8zbnZyEXSRw9.png)

## (b). [30 marks]

### BMM-EM algorithm

The general `BMM-EM` algorithm are shown below(I omit the `main` class, only the `EM` steps), and the preprocessing is included under `em` method :

```python
import numpy as np
from scipy import stats
import time

SEED = 45713
PRNG = np.random.RandomState(SEED)

## NUMBER OF CLUSTERS ##
K = 37
N = 20800
D = 784
####

observations = np.zeros((N,D))

fname = 'test_img.txt'

def _indices(a, func):
	return [i for (i, val) in enumerate(a) if func(val)]

def em(observations, cont_tol, iterations):
	[N, D] = observations.shape
	iteration = 1
	delta_change = 9999

	# Init Model
	pi = np.random.rand(K)
	#mu = [[.5, .5, .5, .5], [.9, .1, .1, .9]]
	mu = np.random.rand(K, D)
	r = np.zeros([N, K]) # soft assignment
	weight = np.zeros(K)

	# Main loop
	while iteration <= iterations:
		# E step
		print('Iter:',iteration)
		which_e = 0
		for ii in range(0, N):
			observation = observations[ii]
			for kk in range(0, K):
				weight[kk] = pi[kk]
				for jj in range(0, D):
					weight[kk] *= mu[kk][jj]**int(observation[jj]) * (1-mu[kk][jj])**(1-int(observation[jj]))
					which_e +=1
					if(which_e % 78600000 == 0):
						print('E Step:'+ str(int(which_e /78600000)) +'/6')
			if(sum(weight) != 0):
				r[ii, :] = weight / sum(weight)

		# M step
		nk = [sum(r[:, ii]) for ii in range(K)]

		new_mu = np.zeros([K, D])
		which_m = 0
		for kk in range(0, K):
			mean = np.zeros(D)
			for ii in range(0, N):
				mean += r[ii, kk]*int(observations[ii])
				which_m +=1
				if(which_m % 78600000 == 0):
						print('E Step:'+ str(int(which_m /78600000)) +'/6')
			new_mu[kk] = mean / nk[kk]
		pi = nk / sum(nk) 
```

### MapReduce Implementation

`Mapper.py`, which plays the role of E step:

```python
#!/usr/bin/env python3

import numpy as np
import os
import sys

SEED = 45713
PRNG = np.random.RandomState(SEED)

K = 37
D = 784

pi = np.zeros(K)
mu = np.zeros((K,D))
r = np.zeros(K)
weight = np.zeros(K)

fname = 'parameters.txt'

if os.path.exists(fname):
        if os.path.getsize(fname) == 0:
            with open(fname,"a") as f:
                pi = np.random.rand(K)
                mu = np.random.rand(K, D)
                out_str = ""
                for i in range(0,K):
                    out_str += str(pi[i]) + " "
                out_str = out_str[:-1]
                out_str += ","
                for k in range(0,K):
                    for d in range(0,D):
                        out_str += str(mu[k][d]) + " "
                out_str = out_str[:-1]
                out_str += "\n"
                f.write(out_str)
                exit()

with open(fname) as f:
    lines = [line.rstrip('\n') for line in open(fname)]
    for line in lines:
        line = line.strip()
        str_1, str_2 = line.split(',')
        pis = str_1.split()
        mus = str_2.split()
        for k in range(0,K):
            pi[k] = float(pis[k])
            for d in range(0,D):
                mu[k][d] = float(mus[k*784+d])

for line in sys.stdin:
    line = line.strip()
    header, line = line.split(':')
    header, digit = header.split()
    observation = line.split()

    for kk in range(0, K):
        weight[kk] = pi[kk]
        for jj in range(0, D):
            weight[kk] *= mu[kk][jj]**int(observation[jj]) * (1-mu[kk][jj])**(1-int(observation[jj]))

    for k in range(0,K):
        if(sum(weight) != 0):
            r[k] = weight[k] / sum(weight)

    for k in range(0,K):
        str_pass = ""
        for pixel in observation:
            str_pass = str_pass + str(pixel) + " "
        str_pass = str_pass[:-1]
        str_pass += ',' + str(r[k])
        print('%s\t%s' % (str(k), str_pass))
```

`Reducer.py`, which plays the role of M step:

```python
#!/usr/bin/env python3

import numpy as np
import os
import sys

K = 37
D = 784
N = 20800

r = np.zeros(K)
pi = np.zeros(K)

current_index = None
count_score = 0

fname = 'parameters.txt'

for line in sys.stdin:
	line = line.strip()
	index, str_pass = line.split('\t')
	observation, score = str_pass.split(',')
	observation = observation.split()

	if (index == current_index):
		count_score += float(score)
		sum_pre = [pre+score*pixel for pre,pixel in zip(sum_pre, observation)]
	else:
		if current_index:
			pi[index] = count_score/N
			sum_pre = [x/count_score for x in sum_pre]
			print str(sum_x).lstrip('[').rstrip(']')
			current_index = index
			count_score = score
			sum_pre = [pre*score for pre in observation]

if (current_index == index):
	pi[index] = count_score/N
	sum_pre = [x/count_score for x in sum_pre]
	print str(sum_x).lstrip('[').rstrip(']')

print(str(pi).lstrip('[').rstrip(']'))
```

But I fail to run  out the result...