---
layout:     post
title:      "Big Data Review"
subtitle:   " \"CMSC5741 Review Notes\""
date:       2020.11.09 23:29:00
author:     "Wuy"
header-img: "img/bigdata.jpg"
catalog: true
tags:
    - Big Data

---

> *"Keep Learning Big Data"*

# 01. Introduction

1. Characteristics of Big Data: Volume, Velocity, Variety, Veracity.
2. Definition of Big Data Analytics: A process of inspecting, cleaning, transforming, and modeling big data with the goal of discovering useful information, suggesting conclusions, and supporting decision making.
3. Pitfall in big data analytics: 邦弗朗尼原理：假定人们有一定量的数据并期望从该数据中找到某个特定类型的事件，即使数据完全随机，也可以期望该类型事件会发生。 邦弗朗尼校正定理给出一个统计学上可行的方法来避免在搜索数据时出现的大部分“臆造”的正响应。 例如：如果考察的时间和范围过广，会很容易发现一些人同住一家酒店，而两者没有什么关系。

# 02. MapReduce

1. MapReduce 存储架构：HDFS、GFS，下图为GFS架构示例：

   ![image-20201108000330645](https://i.loli.net/2020/11/08/rH1j6GMTCO5AUmw.png)

2. The Hadoop Distributed File System (HDFS) is a natural solution to cope with the scale and failure faced by large clusters：

   - Distributed File System（A HDFS instance may consist of thousands of server machines, each storing **part** of the file system’s data.）

   - Provides global file namespace

   - Replica to ensure data recovery

   - Detection of faults in thousands of components and quick, automatic recovery from them are a core architectural goal of HDFS 

3. Data Characteristics:

   - Streaming data access
   - Batch processing rather than interactive user access
   - Write-once-read-many: a file, once created, written and closed, need not be changed

4. HDFS Architecture: NameNode is a file system namespace allow user to store files in it. DataNodes store splited blocks of files, and serves read, write requests, performs block creation, deletion, and replication upon instruction from NameNode：

   ![image-20201108113931742](https://i.loli.net/2020/11/08/E6jQslCa53ymci4.png)

5. Data Replication：

   - Each file is a sequence of blocks.
   - All blocks in the file, except the last, are of the same size.
   - Blocks are replicated for fault tolerance.
   - Block size and replicas are configurable per file.

6. Principles of replica selection: 尽量减少带宽消耗和延迟、优先选择可读节点、尽量选择本地数据中心（如果有远程数据中心）.

7. Safemode of NameNode: 安全模式是HDFS所处的一种特殊状态，在这种状态下，文件系统只接受读数据请求，而不接受删除、修改等变更请求。在NameNode主节点启动时，HDFS首先进入安全模式，DataNode在启动的时候会向namenode汇报可用的block等状态，当整个系统达到安全标准时，HDFS自动离开安全模式。如果HDFS出于安全模式下，则文件block不能进行任何的副本复制操作，因此达到最小的副本数量要求是基于datanode启动时的状态来判定的，启动时不会再做任何复制（从而达到最小副本数量要求）.

8. NameNode keeps image of entire file system namespace(FsImage) and Editlog, and stores the copy of FsImage as a checkpoint in case of crash. 4GB of local RAM is sufficient to store the above data structures.

9. DataNode has no knowledge about HDFS filesystem and DataNode does not create all files in the same directory. It only generates reports when the file system starts up and sends its report to the NameNode.

10. MapReduce steps overview: 

    - Sequentially read a lot of data(a set of key-value pairs)
    - Map: Extract something you care about
    - Group by key: shuffle and sort
    - Reduce: Aggregate, summarize, filter or transform
    - Write the result

11. MapReduce workflow(DataNodes also serve as compute servers):

    ![image-20201108150341704](https://i.loli.net/2020/11/08/ye7EZcrJoisRCL8.png)

12. Dataflow:

    - Schedulers tend to schedule tasks "close" to  physical storage location of input data
    - Intermedia results are stored on local file system of Map and Reduce workers
    - Output is often the input of another MapReduce job

13. Mater node:

    - Task status: (idle(闲置), in-progress, completed)

    - Idle tasks get scheduled as workers become available

    - When a map task completes, it sends the master the location and sizes of its $R$ intermediate files, one for each reducer 

    - Master pushes this info to reducers
    - Master ping workers periodly to detect failures

14. Failures: 

    - All completed and in-progress tasks are reset to idle once Map workers fail and Reduce workers will be notified
    - Only in-progress tasks are reset to idle and restarted

15. Rule of mappers/reducers:

    - Make mappers much larger than nodes' amount
    - One chunk per mapper is common
    - Ususally mappers are larger than reducers

16. Combiners:

    - Only if reducers are commutative and associative, we can use combiners to pre-aggregate values in the mapper to save network times

17. Partitioners:

    - Inputs to map tasks are created by contiguous splits of input file
    - Reducer needs to ensure that records with the same intermediate key end up at the same worker
    - System has a default partition function: $Hash(key)\,mod\,R$
    - Sometimes useful to override the hash function: E.g., $Hash(hostname(URL))\;mod\;R$ ensures `URLs` from a host end up in the same output file

18. Hadoop Streaming command distribute the `mapper.py` and `reducer.py` to every task tracker

19. Costmeasure of MapReduce: 

    - Communication cost: input file size + 2 × (sum of the sizes of all files passed from Map processes to Reduce processes) + the sum of the output sizes of the Reduce processes. 
    - Elapsed communication cost: the sum of the largest input + output for any map process, plus the same for any reduce process 
    - (Elapsed) computation cost, which is linear in the input + output size, so it is like communication cost
    - The big-O notation is not always useful(adding more machines is always an option)
    - Elapsed cost is wall-clock time using parallelism

# 03. Frequent Itemsets

1. Computation Model: the true cost is the number of disk $I/O$  and we measure it by the number of passes an algorithm makes over the data
2. A-Priori algorithm:
   - Key idea: monotonicity: If a set of items $I$ appears at least $s$ times, so does every subset $J$ of $I$
   - Contrapositive for pairs: If item $i$ does not appear in $s$ baskets, then no pair including $i$ can appear in $s$ baskets

# 04. Locality Sensitive Hashing

1. **Jaccard相似系数**（*Jaccard similarity coefficient*）又称**Jaccard系数**(*Jaccar Index*): 两个集合A和B的交集元素在A，B的并集中所占的比例，称为两个集合的Jaccard相似系数，用符号$J(A, B)$表示, **Jaccard相似度=Jaccard相似系数**, 与杰卡德相似系数相反的概念是**Jaccard距离(***Jaccard Distance*)。Jaccard相似系数与Jaccard距离之间的关系：Jaccard距离是Jaccard相似系数的补集，被定义为**Jaccard距离=1-Jaccard相似系数**

2. 局部敏感哈希的处理流程示例：

   ![image-20201108202454716](https://i.loli.net/2020/11/08/WXsUB6LmJovpjZi.png)

3. Shingles: A *k-shingle* (or *k-gram*) for a document is a sequence of k tokens that appears in the doc, and shingle can be represented as a set or a bag, normally , we choose a set. k should be relatively large so that they could appear in both two documents rarely

4. Equivalently, each document is a 0/1 vector in the space of *k-shingles*: 

   - Each unique shingle is a dimension
   - Vectors are very sparse

5. Working assumption: Documents that have lots of shingles in common have similar text, even if the text appears in different order

6. Min-Hashing:

   - The goal is to find a hash function that If sim(C_1,C_2) is high, then with high prob. h(C_1 )=h(C_2) and If sim(C_1,C_2) is low, then with high prob. h(C_1 )≠h(C_2)

   - Not all similarity metrics have a suitable hash function

   - Define a “hash” function h_π (C) = the number of the first (in the permuted order π) row in which column *C* has a value 1 and use sevral independent hash functions to create a signature of a column

   - How to prove that minhash function h fulfill that: $Pr[h(C1) = h(C2)] = sim(C1, C2)$ :

     ![image-20201108222510794](https://i.loli.net/2020/11/08/sQ6FLMzWXHPpVBI.png)

7. Tricks in Min-Hashing:

   - Permuting rows even once is prohibitive
   - Universal hashing: h_(a,b) (x)=((a∙x+b)mod p)  mod N, where *a, b* are random integers, *p* is a prime number (*p*>N), and *a* and *p* must be relative prime for a true permutation

8. Locality Sensitive Hashing: 

   - Goal is to find documents with Jaccard similarity at least *s* (for some similarity threshold)
   - General idea: Use a (hash) function *f(**x,y**)* that tells whether x and y is a candidate pair: a pair of elements whose similarity must be evaluated
   - For minhash matrices: Hash columns of signature matrix to many buckets and then each pair of documents that hashes into the same bucket is a candidate pair (for further examination)
   - We divide matrics into b bands with r rows. And for each band, hash its portion of each column to a hash table with k buckets and make k as large as possible so that two vectors hashing to the same bucket only if they are identical
   - Candidate column pairs are those that hashing to the same bucket with at least one band
   - It is only for simplifying analysis, not for the correctness of algorithm

9. Analysis of the LSH:

   Suppose we use b bands of r rows each, and suppose that a particular pair of documents have Jaccard similarity s. Recall from Section 3.3.3 that the probability the minhash signatures for these documents agree in any one particular row of the signature matrix is s. We can calculate the probability that these documents (or rather their signatures) become a candidate pair as follows:

   - The probability that the signatures agree in all rows of one particular band is $s^r$ .
   - The probability that the signatures disagree in at least one row of a particular band is $1 - s^r$.
   - The probability that the signatures disagree in at least one row of each of the bands is $(1 − s^r )^b$ .
   - The probability that the signatures agree in all the rows of at least one band, and therefore become a candidate pair, is $1 - (1 − s^r )^b$
   - If the numer of band and row is low, the number of false positives would go down and number of false negatives would go up
   - We need to check in main memory that **candidate pairs** really do have **similar signatures**

10. LSH函数族：
    - Theory leaves unknown what happens to pairs that are at distance between d_1 and d_2
    - OK for Minhash, others, but must be part of LSH-family definition

# 05. Mining Data Streams

1. Problems on Data Streams: 
   - Sampling data from a stream
   - Queries over sliding windows
   - Filtering a data stream

2. Naive approach: Suppose each user issues s queries once and d queries twice (total of *s*+2d queries), sample rate is p. And the sample-based answer is: $dp^2/(sp + dp^2 + 2p(1-p)d)$

3. Generalized Solution:

   - Pre-Claim: This algorithm maintains a sample *S* with the desired property, i.e., each item is in the sample *S* with equal prob.

   - The probability that tuple is still in S is $s/(n + 1)$, the proof is: 

     ![image-20201109150139363](https://i.loli.net/2020/11/09/m3TDbsqgSOXRfPY.png)

4. DGIM method:

   - It store $O(log^2N)$ bits per stream
   - 窗口内1数目的估计错误率不高于50%
   - A *bucket* in the DGIM method is a record consisting of: the timestamp of its end is $O(logN)$bits and the number of 1 between its beginning and end is $O(log(logN))$bits
   - We need $O(logN)$buckets and every buckets represent $O(logN)$bits, so the whole space consumed is $O(log^2N)$
   - Updating the buckets: When a new bit comes in, drop the last (oldest) bucket if its end-time is prior to *N* time units before the current time
   - When the estimated number of 1 is smaller than true number, the error ratio is at most $2^{r - 1}/(2^r - 1) ≈ 0.5$; else if the estimated number of 1 is larger than true number, the error ratio is also at most 0.5
   - Can we use the same trick to answer queries “How many 1s in the last *k*?” where *k* < *N*?: Find earliest bucket B that at overlaps with *k*. Number of 1s is the sum of sizes of more recent buckets + ½ size of B
   - How to reduce the error: Instead of maintaining 1 or 2 of each size bucket, we allow either *r*-1 or *r* for *r* > 2 and the error is at most 1/r

5. 流过滤：

   - A typical solution: hashing the "good" values to bit buckets and exam the data stream, but it will create false positives without false negatives
   - Considering the throwing darts model, let $m$ the number of darts and $n$ the number of targets, then the fraction of 1s in the array *B* equlas to  probability of false positive, which is $1 - e^{-m/n}$

6. Bloom Filter:

   - Using k hash functions and only if hash value in all of the k buckets, we declare that it belongs to "good"
   - we have $k*m$ darts, so the fraction of 1 in one hash function is $1 - e^{-km/n}$ and the false positive ratio is $(1 - e^{-km/n})^k$ 
   - we should make k small to lower the false positives, such as $(n/m) \,ln2$
   - Summary: it has no false negatives and is suitable for hardware implementation due to paralized hash functions

7. Flajolet-Martin Approach:

   - Pick a hash function *h* that maps each of the *n* elements to at least $log_2N$ bits

   - For each stream element *a*, let *r*(*a*) be the number of trailing 0s in *h*(*a*): *r(a)* = position of first 1 counting from the right

   - Record *R* = the maximum *r*(*a*) seen

   - Estimated number of distinct elements = $2^R$

   - Proof:

     ![image-20201109233405775](C:/Users/MSI-NB/AppData/Roaming/Typora/typora-user-images/image-20201109233405775.png)

   - But it does not always work, because when R -> R + 1, the value doubles and probability halves, so partition your samples into small groups and take the average of groups then take the median of the averages

   - But the above method still has fallback, because the result is always the power of 2, and it is not so precise

# 06. Scalable Clustering

1. Methods of Clusting:

   - Hierarchical: Agglomerative (bottom up) with complexity $O(n^2log(n))$ using priority queue or $O(n^3)$ without using priority queue and Divisive (top down) with complexity $O(2^n)$
   - Flat (Point assignment): Using K-Means algorithm

2. K-Means clustering:

   - 伪代码：

     ![image-20201110012625283](https://i.loli.net/2020/11/10/CO95K2HYFXdTnGB.png)

   - Pros: Simple and scalable; Cons: takes too many iterations and sensitive to the choose of initialized points

   - K-Means++: 使用轮盘法找出下一个聚类中心，其余算法与K-Means一致

   - K-Means||: To avoid oversample of K-Means++, 主要思路是改变每次遍历时候的取样规则，并非按照K-Means++算法每次遍历只获取一个样本，而是每次获取K个样本，重复该取样操作$O(logN)$次**(N是样本的个数)**，然后再将这些抽样出来的样本聚类出K个点，最后使用这K个点作为K-Means算法的初始聚簇中心点

3. BFR Algorithm:

   - 用来对高维欧氏空间的数据进行聚类， 并且簇的形状必须满足以质心为期望的正态分布
   - 3 sets of points which we keep track of: Discard set, Compression set, Retained set
   - Variance of a cluster’s discard set in dimension *i* is: $SUMSQ_i/N - (SUM_i/N)^2$ 
   - Using Mahalanobis distance ($\sqrt{\sum_i^d((x_i- d_i)/\sigma_i)^2}$) to decide whether two compression sets need to be combined to one

# 07. Dimensionality Reduction

1. SVD: 

   - It gives the minimum reconstruction error(minimum sum of squares of projection error)

   - Reduce dimension by set smallest sigular value to 0 and make origin matrics' rank lower, 保留足够的奇异值使得剩下的奇异值的平方和占比80%到90%

   - 复杂度：$O(min(nm^2, n^2m))$ 

   - The relationship to Eigen-decomposition:

     ![image-20201110035642151](https://i.loli.net/2020/11/10/kB9KUwq8TtonVyE.png)

   - Drawbacks: **Interpretability problem**, **Lack of** **sparsity**

2. CUR:

   - Complexity: $O(mn)$

   - 伪代码：

     ![image-20201110043941198](https://i.loli.net/2020/11/10/m7xBpbGXojSMHYe.png)

   - 计算过程：

     ![image-20201110044358095](https://i.loli.net/2020/11/10/D73ufEeodrsZI1G.png)

3. PCA(主成成分分析)：

   - 将原始数据按列组成n行m列矩阵X
   - 将X的每一行（代表一个属性字段）进行零均值化（去平均值），即减去这一行的均值
   - 求出协方差矩阵 $C = {1 \over m}XX^T$
   - 求出协方差矩阵的特征值及对应的特征向量
   - 将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P（保留最大的k个特征向量）
   - $Y = PX$ 即为降维到K维后的数据

# 08. Recommender Systems / Matrix Factorization

1. LU分解：主要利用高斯消元法对单位对角矩阵进行变换得到L，对原矩阵进行变换得到U

2. PMF: 

   - 推导步骤：

   ![image-20201110155737677](https://i.loli.net/2020/11/10/9jcz31rpaLbFiYX.png)

   ![image-20201110155857265](https://i.loli.net/2020/11/10/CEmVi2YTvK8o6an.png)

   ![image-20201110155918399](https://i.loli.net/2020/11/10/iJlfD35CFOdNmPI.png)

   ![image-20201110155934837](https://i.loli.net/2020/11/10/4qoviD1dCrsxbUB.png)

   ![image-20201110155955504](https://i.loli.net/2020/11/10/S1CEBNXyLkdmKY4.png)

   - 伪代码：

   ```
   Input: the number of latent factor K, the learning rata eta, 
   regularization parameters lambda_1,lambda_2, the max iteration Step,
   and the rating matrix R
   
   Initialization: Initialize a random matrix for user matrix U and item matrix V
   
   for t = 1, 2,...Step do
       for (u,i,r) in R
           make prediction pr=Ui^T*Vj
           error e=r-pr
           update Ui and Vj by (5) and (6)
           the algorithm suffers a loss (Ui, Vj, r)
       end for
   end for
   ```

3. 非负矩阵分解（NMF）：

   ![image-20201110160405870](https://i.loli.net/2020/11/10/hUcYSA374HIoLKs.png)

   ![image-20201110160428515](https://i.loli.net/2020/11/10/ycQu3q89OUMbsJl.png)